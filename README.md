# Гибридный алгоритм обучения с подкреплением с ACO и SMA

Обучение агента в двумерной решётке с препятствиями
с использованием гибридного подхода:

- обучения с подкреплением (Q-learning),
- муравьиного алгоритма (Ant Colony Optimization, ACO),
- алгоритма слизевика (Slime Mould Algorithm, SMA).

Основная цель — исследовать, может ли использование
метаэвристических алгоритмов улучшить скорость сходимости,
устойчивость и качество обучения RL-агента,
особенно в динамической среде.


## Постановка задачи

Агент должен найти путь из стартовой клетки в целевую
на двумерной сетке с препятствиями.

Особенности среды:
- статические и динамические препятствия;
- штрафы за столкновения со стенами;
- награда за достижение цели;
- дополнительное поощрение за приближение к цели
  (на основе манхэттенского расстояния).

## Используемые алгоритмы

### Обучение с подкреплением
- Табличный Q-learning
- ε-жадная стратегия выбора действий
- Плавное уменьшение ε

### Муравьиный алгоритм (ACO)
- Используется для поиска эвристического пути
- Применяется для:
  - предварительного заполнения Q-таблицы
  - перестроения стратегии при низкой успешности обучения

### Алгоритм слизевика (SMA)
- Популяционный метаэвристический алгоритм
- Используется для оптимизации гиперпараметров:
  - коэффициента обучения (α),
  - коэффициента дисконтирования (γ)

`Hybrid_RL.py` Основной цикл обучения и экспериментов
`SMA_algorithm.py` Реализация алгоритма Slime Mould Algorithm
`ACO_for_RL.py` Муравьиный алгоритм для поиска путей
